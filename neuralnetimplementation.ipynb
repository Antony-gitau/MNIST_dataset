{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralnetimplementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1MBnhkZ/QChuSLk/sSS8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antony-gitau/MNIST_dataset/blob/main/neuralnetimplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains an algorithm that helps machines recognize handwritten digits. The motivation to implement this algorithm comes from reading [Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap1.html) book which walked me through the core concepts of deep learning using MNIST dataset as the hands on exercise.\n",
        "\n",
        "The goals of this notebook:\n",
        "---\n",
        "1. Write shothand notes on neural network concepts I learn in the book\n",
        "2. Implement and test the algorithm that recognizes/classifies handwritten digits."
      ],
      "metadata": {
        "id": "CWdzHLG0k-_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Perceptron\n",
        "---\n",
        "\n",
        "Basically, perceptron weigh inputs and make a corresponding decision.\n",
        "\n",
        "example:\n",
        "\n",
        "If this weekend there is a hike coming up and i need to decide whether to go or not. \n",
        "the \"inputs\" or factors to consider while making the decision to go or not would be:-\n",
        " 1. Will my sister join me\n",
        " 2. Will I have a private means of travel to the hiking destination\n",
        " 3. Is the weather favourable for hiking\n",
        "\n",
        "If my sister joining me has significance of 3, and if I have a private means has a significance of 8 and if the weather is favourable takes a significance of 5. The significance is just how important that factor is to determining my going or not. Conventionally, the significance is the weight and the factor is the input. For me to go, I might decide the threshold is a 6. Threshold is conventionally known as bias. This means i will go to the hike if there is a private transport regardless of the other two inputs. Private transport matters to me a big deal. Now, supposing, i changed the threshold to 9 from 6. The decision paradigm changes. I will now go to the hike from atleast a combination of two inputs. We could also change the significance of the inputs and as such, we would have a different decision classifier.\n",
        "\n",
        "downside of perceptron:-\n",
        " 1. changing the weights/bias dont always result to the desired output.\n",
        " 2. A way to modify the weight and bias doesnt exist in a real life situation unlike our metaphor above.\n",
        " 3. There is just no way to teach a perceptron how to learn.\n",
        "\n",
        "This is why we have \n",
        "\n",
        "Sigmoid neuron.\n",
        "---\n",
        "The only major difference between a perceptron and a sigmoid neuron is that the output can be anything between 0 -> 1 unlike the 0 or 1 situation for the perceptron. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The architecture of a neural network \n",
        "---\n",
        "\n",
        "well, a neural network has three main parts:\n",
        "- input layer\n",
        "- hidden layer\n",
        "- output layer\n",
        "\n",
        "input layer - contains the input data. for example if the data i am feeding the neural network is the image of a cat of 28 by 28 units, then my input neuron (which makes the input layer) will is 784 neurons. \n",
        "\n",
        "output neuron in this case will be say not a cat image if output neuron is less than 0.5 and it's a cat image if the output is greater than 0.5.\n",
        "\n",
        "hidden layer just means that the neuron is neither input nor output. \n",
        "\n",
        "when designing a neural network, the input and output layers are relatively easy to determine, unlike the hidden layers. There exists many architectures for determining the hidden layers. The most common onces are referred to as feedforward neural networks as the output of one hidden layer feeds to the next layer, and we dont have a loop (a layer feeding the preceeding one except the recurrent neural network)\n",
        "\n",
        "---\n",
        "A neural network that can classify handwritten digits.\n",
        "---\n",
        "\n",
        "input layer - 28 by 28 pixel of handwritten digit images. This makes a 784 input neuron.\n",
        "output layer - 10 output neuron. Every neuron will have an output corresponding to the input.\n",
        "hidden layer - contains one hidden layer with n neurons. n is a number that will represent the number of neurons as we will experiment on different numbers.\n",
        "\n",
        "we will use gradient descent algorithm to train the model. \n",
        "we will use quadratic cost function \n",
        "\n",
        "the MNIST data was made available by National Institute of Standards and Technology. it contains 60,000 training data and 10,000 test data.\n",
        "\n",
        "$C(w,b)≡\\frac{1}{2n}∑_x ∥y(x)−a∥^2$     --------------------> (1)\n",
        "\n",
        "c - quadratic cost function also called mean square error (MSE)\n",
        "\n",
        "w- weights \n",
        "\n",
        "b - biases\n",
        "\n",
        "n - number of inputs\n",
        "\n",
        "x - input data\n",
        "\n",
        "a - outputs of the data\n",
        "\n",
        "∑ - summation of all the differences \n",
        "\n",
        "so for an input like 6, the output vector $y(x) = (0,0,0,0,0,0,1,0,0,0)^T$\n",
        "\n",
        "---\n",
        "Learning with gradient descent.\n",
        "---\n",
        "\n",
        "core issues to discuss:\n",
        "- cost function\n",
        "- learning rate\n",
        "- gradient descent : batch, stochastic, mini-batch gradient descents.\n",
        "\n",
        "\n",
        "cost function -> is basically a function with many variable (weights and biases in neural network) where output (predictions) is determined by those input. For example in the case of MNIST dataset we are going to use equation(1) called the quadratic cost function(Mean Squared Error). The goal is to get to the minimum point of the function( $c(w,b) = 0$ ). That means the output the function is giving is the same as the input - which is the goal of prection, right? we want the digit the function receives as the input is the same given at the output. \n",
        "\n",
        "learning rate -> these are basically the small positive steps towards the global minumum. Basically similar to \" strides down hill\".\n",
        "\n",
        "Gradient descent is just a strategy for searching the global minimum of a function. \n",
        "\n",
        "Global minimum is basically the \"lowest\" point of a function. remember that a function could have a number of minimum points ( points where the function is 0) but there could be a minimum point which is at the lowest point on the cartesian plane. \n",
        "\n",
        "batch gradient -> the strategy here is basically taking the whole training set and finding the gradient of the inputs (training data points) with an aim to get to the global minimum. obviously, when the training data is too large, we experience slow training - too large could be hundred of thousands of data points.\n",
        "\n",
        "stochastic gradient descent -> well here we want to randomly take a bunch of training data (mini batch) and then individually find the gradient of those inputs. from the training average of a this random sample, we can then approximate the overall gradient which is our goal - to get the overall gradient and so \"walk us down hill\" to the global minimum. \n",
        "\n",
        "mini-batch gradient descent -> here we are using a small random sample of the training set to do training (training is basically finding the gradient of the samples in the samples) \n",
        "\n",
        "learning scheduler -> the function that controlls the rate of \"small positive steps towards the global minimum\" simply the learning rate.\n",
        "\n",
        "epoch -> the iterations of training that happen during training until all the samples have been trained.\n",
        "\n",
        "---\n",
        "\n",
        "Implementing the neural net to classify the digits.\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XbIOsDxB_S83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POXVtxjZhWNY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n"
      ],
      "metadata": {
        "id": "QwCzU9urpaLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train.info()"
      ],
      "metadata": {
        "id": "OFpJPMpIpeQk",
        "outputId": "73d3a540-a527-4331-8b37-e50a304da93d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19999 entries, 0 to 19998\n",
            "Columns: 785 entries, 6 to 0.590\n",
            "dtypes: int64(785)\n",
            "memory usage: 119.8 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backprop:\n",
        "\n",
        "Backprop is basically the function that computes gradient descent.\n",
        "How quickly does the cost function change with respect to any weight or bias in the network. The way to find how fast, is by computing partial derivatives of the cost function in respect to weights and biases.\n",
        "\n",
        "$w_{jk}^l$\n",
        "\n",
        "- $k^{th}$ neuron\n",
        "- $(l-1)^{th}$ layer of $j^{th}$ neuron\n",
        "\n",
        "[illustration of the denotation above](http://neuralnetworksanddeeplearning.com/images/tikz16.png)\n",
        "\n",
        "$b^j_j$ this is the notation of the bias term.\n",
        "\n",
        "- $j^{th}$ neuron in the $l^{th}$ layer. \n",
        "\n",
        "vectorization - applying a function such as sigmoid to all the elements in vector. eg the weight vector of a neural network.\n",
        "\n",
        "Assumptions about the cost function that allow backprop to be applicable:\n",
        "\n",
        "1. the cost function can be written as an average $ C = \\frac{1}{n}∑_xC_x$. \n",
        "\n",
        "- $C_x$ is cost function, for individual training examples, $x$.\n",
        "\n",
        "Backprop is computing the partial derivatives for every training example then averaging over all training examples to get the effect of weights and biases of the entire layer.\n",
        "\n",
        "2. The cost function can be represented as a function of outputs from the neural network.\n",
        "\n",
        "fundamental equations behind backprop:\n",
        "\n",
        "- based on operations such as vector addition, multiplication etc.\n",
        "\n",
        "hadamard product - suppose s\n",
        " and t\n",
        " are two vectors of the same dimension. Then we use s⊙t\n",
        " to denote the elementwise product of the two vectors. This element wise operation is called hadamard."
      ],
      "metadata": {
        "id": "Z3Q3g5nUxbp5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PhA81Voc9YjQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}